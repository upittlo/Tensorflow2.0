{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e16833",
   "metadata": {},
   "source": [
    "### Create a character level LM\n",
    "\n",
    "- The content might not make sense\n",
    "- It's just a code template \n",
    "\n",
    "\n",
    "reference: \n",
    "https://www.tensorflow.org/text/tutorials/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d95fc10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aec9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download dataset\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5739a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "### Read dataset\n",
    "\n",
    "# Read, then decode for py2 compat.\n",
    "\n",
    "text = open(path_to_file,'rb').read().decode(encoding = 'utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "862a695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Take a look at first 250 characters\n",
    "\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670c22c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "### Number of unique characters \n",
    "\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09410b",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "- vectorize text: convert string to numerical representation\n",
    "- The tf.keras.layers.StringLookup layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d247076d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### split string into list\n",
    "### convert to numeric\n",
    "\n",
    "example_texts = ['abcdefg','xyz']\n",
    "chars = tf.strings.unicode_split(example_texts,input_encoding = 'UTF-8')\n",
    "chars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dbad338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary = list(vocab),mask_token = None)\n",
    "\n",
    "### check example \n",
    "\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b545f",
   "metadata": {},
   "source": [
    "### Convert numeric back to char\n",
    "\n",
    "- invert:  invert this representation and recover human-readable strings\n",
    "-  get_vocabulary() method of the tf.keras.layers.StringLookup layer so that the [UNK] tokens is set the same way\n",
    "\n",
    "\n",
    "- tf.strings.reduce_join: join char back to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48045817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary = ids_from_chars.get_vocabulary(), invert = True, mask_token = None)\n",
    "\n",
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb5210b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### join back to string\n",
    "\n",
    "tf.strings.reduce_join(chars,axis = 1).numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3591e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids),axis = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0453c",
   "metadata": {},
   "source": [
    "### Create training data\n",
    "\n",
    "- next word prediction\n",
    "- right shift one char as output\n",
    "\n",
    "- Use batch method with tf.data.Dataset.from_tensor_slices to create sequences of desired size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae8f549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids\n",
    "\n",
    "\n",
    "### convert the text vector into a stream of character indices.\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "153083af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "\n",
    "## 資料合併成一個個batch, 長度為seq_length+1\n",
    "\n",
    "sequences = ids_dataset.batch(seq_length+1,drop_remainder = True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15a2cdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "## easier to see what this is doing if you join the tokens back into strings\n",
    "\n",
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cbeae",
   "metadata": {},
   "source": [
    "### Training pairs (start from here)\n",
    "- For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9fa77a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    \n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e93655c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "Input : b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "Target: b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "Input : b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n",
      "Target: b\"ow Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(3):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4896d5d",
   "metadata": {},
   "source": [
    "### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a6c3037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 100), dtype=tf.int64, name=None), TensorSpec(shape=(None, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Batch size\n",
    "BATCH_SIZE =64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d873db5c",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "##### keras.Model subclass reference: https://www.tensorflow.org/guide/keras/custom_layers_and_models \n",
    "\n",
    "- Embedding: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;\n",
    "- GRU: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
    "- Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d65ac52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e093b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,rnn_units):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        ### Define layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                      return_sequences = True,\n",
    "                                      return_state = True)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, states = None, return_state = False,training = False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x,training=training)\n",
    "        \n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "            \n",
    "        x,states = self.gru(x,initial_state = states,training = training)\n",
    "        x = self.dense(x,training=training)\n",
    "        \n",
    "        if return_state:\n",
    "            return x,states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa0ec090",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "        vocab_size = vocab_size,\n",
    "        embedding_dim = embedding_dim,\n",
    "        rnn_units = rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587333ae",
   "metadata": {},
   "source": [
    "### Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b825217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "### check the output shape\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47f66086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2065928",
   "metadata": {},
   "source": [
    "#### Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93e7b88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 11, 46,  4, 45, 47,  4, 24, 26, 47, 48, 34,  1,  4, 28,  4, 54,\n",
       "       11, 32, 33, 38, 59,  7, 41, 10, 38, 50, 29,  4, 11,  0, 54, 43,  0,\n",
       "       27, 18, 52, 41,  5, 18, 45, 48, 65, 43, 27, 34, 49, 20, 58, 28, 19,\n",
       "       55, 25, 54, 23, 36, 30, 50, 27, 53, 21, 29, 39, 58,  4, 18, 31, 25,\n",
       "        2,  3, 61,  3,  9,  4, 45, 54, 55, 55, 20,  4, 47, 21, 31, 59, 50,\n",
       "       25, 46, 39, 20,  6, 17, 54, 36, 63, 53,  8, 52, 49, 44, 10],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples = 1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()\n",
    "\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3c49474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'aying he would make his son\\nHeir to the crown; meaning indeed his house,\\nWhich, by the sign thereof '\n",
      "\n",
      "Next Char Prediction:\n",
      " b\"n:g$fh$KMhiU\\n$O$o:STYt,b3YkP$:[UNK]od[UNK]NEmb&EfizdNUjGsOFpLoJWQkNnHPZs$ERL !v!.$foppG$hHRtkLgZG'DoWxn-mje3\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Prediction:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3bb5b",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c538fea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (64, 100, 66) (batch_size,sequence_length,vocab_size)\n",
      "Mean loss:        tf.Tensor(4.189708, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Because your model returns logits, you need to set the from_logits flag.\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "\n",
    "## check loss before training\n",
    "example_batch_mean_loss = loss(target_example_batch,example_batch_predictions)\n",
    "\n",
    "print(\"Prediction shape:\", example_batch_predictions.shape,\"(batch_size,sequence_length,vocab_size)\")\n",
    "print(\"Mean loss:       \",example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4ee3cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.00353"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1cda1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5bb330",
   "metadata": {},
   "source": [
    "#### Configure checkpoints\n",
    "- Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab417ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fde1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "173/173 [==============================] - 8s 33ms/step - loss: 2.7355\n",
      "Epoch 2/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 2.0006\n",
      "Epoch 3/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.7177\n",
      "Epoch 4/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.5541\n",
      "Epoch 5/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.4539\n",
      "Epoch 6/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.3855\n",
      "Epoch 7/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.3317\n",
      "Epoch 8/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.2870\n",
      "Epoch 9/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.2455\n",
      "Epoch 10/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.2062\n",
      "Epoch 11/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.1662\n",
      "Epoch 12/20\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 1.1252\n",
      "Epoch 13/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.0815\n",
      "Epoch 14/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 1.0364\n",
      "Epoch 15/20\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.9873\n",
      "Epoch 16/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 0.9367\n",
      "Epoch 17/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 0.8842\n",
      "Epoch 18/20\n",
      "173/173 [==============================] - 6s 33ms/step - loss: 0.8306\n",
      "Epoch 19/20\n",
      "173/173 [==============================] - 6s 34ms/step - loss: 0.7797\n",
      "Epoch 20/20\n",
      "173/173 [==============================] - 6s 32ms/step - loss: 0.7304\n"
     ]
    }
   ],
   "source": [
    "### Execute the training\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "history = model.fit(dataset,epochs=EPOCHS,callbacks = [checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291d0fb",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n",
    "- tf.random.categorical：https://blog.csdn.net/menghuanshen/article/details/105356239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "197e70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        # logits: 形状为 [batch_size, num_classes]的张量. 每个切片 [i, :]代表对于所有类的未正规化的log概率。\n",
    "        # num_samples: 0维，从每一行切片中抽取的独立样本的数量。\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ca5aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model,chars_from_ids,ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1190b1f",
   "metadata": {},
   "source": [
    "### Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b36f7f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Why, vouchy is most government?\n",
      "If I did fleezan, that the king\n",
      "Shall be my queen, and let him all apart.\n",
      "\n",
      "BIANCA:\n",
      "Ay, marry, goes with me.\n",
      "\n",
      "Justice:\n",
      "By the hollivest man, should seem again to my soul.\n",
      "You'll marry us to old right? What art thou\n",
      "That will not think the heart that there die, what blows?\n",
      "\n",
      "CLARENCE:\n",
      "Thou hast provoked to hang them nothing but a Coriolanus\n",
      "Had propice my honest more rich than any man divired his else\n",
      "But doubt not that the King of Halp's--\n",
      "May not appear, and gall people,\n",
      "To save your loving trimingly ripes in gentle, betrause\n",
      "Hurts you of roop and married; else, my father, in my birth,\n",
      "And the most predrops of them, for there\n",
      "it is now reply. But, sup out with the direst devier\n",
      "Thou hast possession that wanter down, her eye\n",
      "Than a little office with an our state\n",
      "And liberty be thou disposes of fear.\n",
      "\n",
      "TRANIO:\n",
      "Shalt thou be made, hear me in my throat,\n",
      "boys: beholder, his friends: if you whit end o'erthem\n",
      "And hell encounter air usur sin.\n",
      "\n",
      "AUTOLYCUS:\n",
      "O, good \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.0612967014312744\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa916c",
   "metadata": {},
   "source": [
    "### Save for further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "136e7cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000001B4C4679F40>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000001B4C4679F40>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "##tf.saved_model.save(obj, export_dir, signatures=None, options=None)\n",
    "\n",
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3323e7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "My lord perchio.\n",
      "\n",
      "PETRUCHIO:\n",
      "Well, go with me this is my son. Who should say the circumness\n",
      "We shal\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
